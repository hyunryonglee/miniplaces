{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderH5(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.load_size = int(kwargs['load_size'])\n",
    "        self.fine_size=int(kwargs['fine_size'])\n",
    "        self.data_mean=np.array(kwargs['data_mean'])\n",
    "        self.randomize=kwargs['randomize']\n",
    "        \n",
    "        f=h5py.File(kwargs['data_h5'],\"r\")\n",
    "        self.im_set=np.array(f['images'])\n",
    "        self.lab_set=np.array(f['labels'])\n",
    "        \n",
    "        self.num=self.im_set.shape[0]\n",
    "        assert self.im_set.shape[0]==self.lab_set.shape[0], '#images and #labels do not match!'\n",
    "        assert self.im_set.shape[1]==self.load_size,'Image size error!'\n",
    "        assert self.im_set.shape[2]==self.load_size, 'Image size error!'\n",
    "        print('# Images found:', self.num)\n",
    "        \n",
    "        self.shuffle()\n",
    "        self._idx=0\n",
    "        \n",
    "        def next_batch(self,batch_size):\n",
    "            labels_batch=np.zeros(batch_size)\n",
    "            images_batch=np.zeros((batch_size,self.fine_size,self.fine_size,3))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                image=self.im_set[self._idx]\n",
    "                image=image.astype(np.float32)/255.-self.data_mean\n",
    "                if self.randomize:\n",
    "                    flip=np.random.random_integers(0,1)\n",
    "                    if flip>0:\n",
    "                        image=image[:,::-1,:]\n",
    "                        offset_h=np.random.random_integers(0,self.load_size-self.fine_size)\n",
    "                        offset_w=np.random.random_integers(0,self.load_size-self.fine_size)\n",
    "                    else:\n",
    "                        offset_h=(self.load_size-self.fine_size)//2\n",
    "                        offset_w=(self.load_size-self.fine_size)//2\n",
    "                    \n",
    "                    images_batch[i,...]=image[offset_h:offset_h+self.fine_size,offset_w:offset_w+self.fine_size,:]\n",
    "                    labels_batch[i,...]=self.lab_set[self._idx]\n",
    "                    \n",
    "                    self._idx += 1\n",
    "                    if self._idx ==self.num:\n",
    "                        self._idx=0\n",
    "                        if self.randomize:\n",
    "                            self.shuffle()\n",
    "                            \n",
    "                return images_batch, labels_batch\n",
    "            \n",
    "            def size(self):\n",
    "                return self.num\n",
    "            \n",
    "            def reset(self):\n",
    "                self._idx=0\n",
    "                \n",
    "            def shuffle(self):\n",
    "                perm=np.random.permutation(self.num)\n",
    "                self.im_set=self.im_set[perm]\n",
    "                self.lab_set=self.lab_set[perm]\n",
    "                \n",
    "                        \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "load_size=256\n",
    "fine_size=224\n",
    "c=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_mean = np.asarray([0.45834960097,0.44674252445,0.41352266842])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "dropout=0.5\n",
    "training_iters=2000\n",
    "step_display=50\n",
    "step_save=500\n",
    "path_save='alexnet'\n",
    "start_from=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-e31d3f00d25d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-e31d3f00d25d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    'wc1': tf.Variable(tf.random_normal([11 11 3 96], stddev=np.sqrt(2./(11*11*3)))),\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def alexnet(x,keep_dropout):\n",
    "    weights={\n",
    "        'wc1': tf.Variable(tf.random_normal([11 11 3 96], stddev=np.sqrt(2./(11*11*3)))),\n",
    "        'wc2': tf.Variable(tf.random_normal([5 5 96 256], stddev=np.sqrt(2./(5*5*96)))),\n",
    "        'wc3': tf.Variable(tf.random_normal([3 3 256 384], stddev=np.sqrt(2./(3*3*256)))),\n",
    "        'wc4': tf.Variable(tf.random_normal([3, 3, 384, 256], stddev=np.sqrt(2./(3*3*384)))),\n",
    "        'wc5': tf.Variable(tf.random_normal([3, 3, 256, 256], stddev=np.sqrt(2./(3*3*256)))),\n",
    "        \n",
    "        'wf6': tf.Variable(tf.random_normal([7*7*256, 4096], stddev=np.sqrt(2./(7*7*256)))),\n",
    "        'wf7': tf.Variable(tf.random_normal([4096, 4096], stddev=np.sqrt(2./4096))),\n",
    "        'wo': tf.Variable(tf.random_normal([4096, 100], stddev=np.sqrt(2./4096)))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'bc1':tf.Variable(tf.zeros(96)),\n",
    "        'bc2':tf.Variable(tf.zeros(256)),\n",
    "        'bc3':tf.Variable(tf.zeros(384)),\n",
    "        'bc4':tf.Variable(tf.zeros(256)),\n",
    "        'bc5':tf.Variable(tf.zeros(256)),\n",
    "        \n",
    "        'bf6':tf.Variable(tf.zeros(4096)),\n",
    "        'bf7':tf.Variable(tf.zeros(4096)),\n",
    "        'bo':tf.Variable(tf.zeros(100))\n",
    "    }\n",
    "    \n",
    "    # Conv+ReLU+LRN+pool, 224->55->27\n",
    "    conv1=tf.nn.conv2d(x,weights['wc1'],strides=[1,4,4,1],padding='SAME')\n",
    "    conv1=tf.nn.relu(tf.nn.bias_add(conv1,biases['bc1']))\n",
    "    lrn1=tf.nn.local_response_normalization(cov1,depth_radius=5,bias=1.0,alpha=1e-4,beta=0.75)\n",
    "    pool1=tf.nn.max_pool(lrn1,ksize=[1, 3, 3,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    # Conv +ReLU+LRN+Pool, 27->13\n",
    "    conv2=tf.nn.conv2d(pool1,weights['wc2'],strides=[1,1,1,1],padding='SAME')\n",
    "    conv2=tf.nn.relu(tf.nn.bias_add(conv2,biases['bc2']))\n",
    "    lrn2=tf.nn.local_response_normalization(conv2,depth_radius=5,bias=1.0,alpha=1e-4,beta=0.75)\n",
    "    pool2=tf.nn.max_pool(lrn2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    # Conv + ReLU, 13 -> 13\n",
    "    conv3=tf.nn.conv2d(pool2,weights['wc3'],strides=[1,1,1,1],padding='SAME')\n",
    "    conv3=tf.nn.relu(tf.nn.bias_add(conv3,biases['bc3']))\n",
    "    \n",
    "    # Conv + ReLU, 13 -> 13\n",
    "    conv4=tf.nn.conv2d(conv3, weights['wc4'] ,strides=[1,1,1,1],padding='SAME')\n",
    "    conv4=tf.nn.relu(tf.nn.bias_add(conv4,biases['bc4']))\n",
    "    \n",
    "    # Conv + ReLU +Pool, 13 ->6\n",
    "    conv5=tf.nn.conv2d(conv4,weights['wc5'],strides=[1,1,1,1],padding='SAME')\n",
    "    conv5=tf.nn.relu(tf.nn.bias_add(conv5,biases['bc5']))\n",
    "    pool5=tf.nn.max_pool(conv5,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME') \n",
    "    \n",
    "    # FC + ReLU + Dropout\n",
    "    fc6 = tf.reshape(pool5, [-1, weights['wf6'].get_shape().as_list()[0]])\n",
    "    fc6 = tf.add(tf.matmul(fc6, weights['wf6']), biases['bf6'])\n",
    "    fc6 = tf.nn.relu(fc6)\n",
    "    fc6 = tf.nn.dropout(fc6, keep_dropout)\n",
    "    \n",
    "    # FC + ReLU + Dropout\n",
    "    fc7 = tf.add(tf.matmul(fc6, weights['wf7']), biases['bf7'])\n",
    "    fc7 = tf.nn.relu(fc7)\n",
    "    fc7 = tf.nn.dropout(fc7, keep_dropout)\n",
    "\n",
    "    # Output FC\n",
    "    out = tf.add(tf.matmul(fc7, weights['wo']), biases['bo'])\n",
    "    \n",
    "    return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
